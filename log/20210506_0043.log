                          Options
                          seed: 1122
                          batch_size: 1024
                          no_cuda: False
                          max_frames: 500000000
                          buffer_size: 100000
                          update_target: 1000
                          train_freq: 100
                          gamma: 0.99
                          eta: 0.1
                          rl_start: 10000
                          sl_start: 1000
                          num_envs: 3
                          hidden_dim: 256
                          dueling: False
                          multi_step: 1
                          env: SlimeVolley-v0
                          ram: False
                          negative: False
                          against_baseline: False
                          load_model: None
                          save_model: model
                          evaluate: False
                          render: False
                          evaluation_interval: 10000
                          lr: 0.0001
                          max_tag_interval: 3000
                          eps_start: 1.0
                          eps_final: 0.01
                          eps_decay: 30000
                          cuda: True
                          device: cuda:0
[Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32), Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32), Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32)] [Discrete(6), Discrete(6), Discrete(6)]
(12,)
(12,)
Frame: 30000, Avg. Length: 513.2, P0 Avg. Reward: -0.737, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.737, P1 Avg. RL Loss: 0.006
Frame: 60000, Avg. Length: 553.9, P0 Avg. Reward: -0.370, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.370, P1 Avg. RL Loss: 0.005
Frame: 90000, Avg. Length: 553.2, P0 Avg. Reward: 0.222, P0 Avg. RL Loss: nan, P1 Avg. Reward: -0.222, P1 Avg. RL Loss: 0.006
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[-0.068 -0.058  0.075  0.037 -0.026 -0.054]
 [-0.043 -0.038 -0.01   0.037 -0.038 -0.038]
 [-0.057 -0.043 -0.056 -0.022 -0.066 -0.072]
 [-0.125 -0.094  0.029 -0.024 -0.105 -0.074]
 [-0.07  -0.026  0.073  0.012 -0.003 -0.046]
 [-0.133 -0.094 -0.189  0.151  0.009 -0.057]] (array([0., 1., 0., 0., 0., 0.]), array([ 1.,  0.,  0.,  0., -0.,  0.]))
[ 1.  0.  0.  0. -0.  0.]
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[-0.024 -0.025  0.076  0.048 -0.002 -0.016]
 [-0.024 -0.01   0.016  0.051 -0.02   0.003]
 [-0.024 -0.014 -0.023  0.022 -0.022 -0.03 ]
 [-0.115 -0.079  0.042 -0.044 -0.111 -0.056]
 [-0.03   0.002  0.095  0.028  0.012 -0.01 ]
 [-0.142 -0.089 -0.18   0.139  0.026 -0.06 ]] (array([0., 1., 0., 0., 0., 0.]), array([ 1., -0.,  0.,  0.,  0.,  0.]))
[ 1. -0.  0.  0.  0.  0.]
Frame: 120000, Avg. Length: 546.8, P0 Avg. Reward: -0.111, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.111, P1 Avg. RL Loss: 0.006
Frame: 150000, Avg. Length: 561.2, P0 Avg. Reward: -0.167, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.167, P1 Avg. RL Loss: 0.007
Frame: 180000, Avg. Length: 561.2, P0 Avg. Reward: 0.185, P0 Avg. RL Loss: nan, P1 Avg. Reward: -0.185, P1 Avg. RL Loss: 0.008
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[-0.803 -0.707  0.056 -0.408 -0.647 -0.747]
 [-0.785 -0.709 -0.378 -0.438 -0.495 -0.691]
 [-0.809 -0.721 -0.606 -0.226 -0.646 -0.806]
 [-0.725 -0.684  0.007 -0.262 -0.606 -0.725]
 [-0.791 -0.726 -0.332 -0.171 -0.448 -0.801]
 [-0.781 -0.752 -0.691  0.617 -0.341 -0.711]] (array([ 0., -0.,  0.,  1.,  0.,  0.]), array([0.364, 0.   , 0.   , 0.   , 0.   , 0.636]))
[ 0. -0.  0.  1.  0.  0.]
Frame: 210000, Avg. Length: 575.4, P0 Avg. Reward: 0.157, P0 Avg. RL Loss: nan, P1 Avg. Reward: -0.157, P1 Avg. RL Loss: 0.009
Frame: 240000, Avg. Length: 542.6, P0 Avg. Reward: -0.246, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.246, P1 Avg. RL Loss: 0.009
Frame: 270000, Avg. Length: 564.4, P0 Avg. Reward: -0.000, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.000, P1 Avg. RL Loss: 0.010
Frame: 300000, Avg. Length: 564.9, P0 Avg. Reward: 0.020, P0 Avg. RL Loss: nan, P1 Avg. Reward: -0.020, P1 Avg. RL Loss: 0.010
Frame: 330000, Avg. Length: 561.3, P0 Avg. Reward: -0.167, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.167, P1 Avg. RL Loss: 0.009
Frame: 360000, Avg. Length: 550.1, P0 Avg. Reward: 0.222, P0 Avg. RL Loss: nan, P1 Avg. Reward: -0.222, P1 Avg. RL Loss: 0.010
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[ 0.733  0.682  0.712  0.652  0.7    0.741]
 [ 0.753  0.7    0.69   0.71   0.731  0.747]
 [ 0.735  0.698  0.67   0.698  0.705  0.709]
 [ 0.147  0.166  0.201 -0.431 -0.58  -0.084]
 [ 0.754  0.718  0.723  0.735  0.723  0.73 ]
 [-0.257 -0.141 -0.436  0.256  0.399 -0.431]] (array([0., 0., 0., 0., 1., 0.]), array([ 0.,  1.,  0.,  0., -0.,  0.]))
[ 0.  1.  0.  0. -0.  0.]
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[ 0.737  0.648  0.651  0.67   0.664  0.673]
 [ 0.693  0.677  0.684  0.668  0.657  0.698]
 [ 0.681  0.683  0.652  0.624  0.643  0.624]
 [ 0.1    0.151  0.178 -0.385 -0.571 -0.071]
 [ 0.698  0.691  0.7    0.699  0.665  0.691]
 [-0.256 -0.131 -0.418  0.227  0.375 -0.418]] (array([0., 0., 0., 0., 1., 0.]), array([ 0., -0.,  0.,  0.,  1.,  0.]))
[ 0. -0.  0.  0.  1.  0.]
Frame: 390000, Avg. Length: 541.9, P0 Avg. Reward: 0.649, P0 Avg. RL Loss: nan, P1 Avg. Reward: -0.649, P1 Avg. RL Loss: 0.009
Not a valid distribution from Nash equilibrium solution.
1.0 0.9999999999999998
[[ 0.677  0.673  0.732  0.647  0.72   0.73 ]
 [ 0.738  0.699  0.716  0.717  0.699  0.717]
 [ 0.743  0.678  0.675  0.678  0.7    0.668]
 [ 0.14   0.153  0.194 -0.425 -0.561 -0.09 ]
 [ 0.708  0.704  0.73   0.708  0.717  0.721]
 [-0.245 -0.16  -0.449  0.255  0.408 -0.399]] (array([0., 0., 0., 0., 1., 0.]), array([-0.,  1.,  0.,  0.,  0.,  0.]))
[-0.  1.  0.  0.  0.  0.]
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[ 0.407  0.369  0.384  0.378  0.407  0.385]
 [ 0.38   0.383  0.436  0.406  0.45   0.398]
 [ 0.386  0.385  0.392  0.386  0.405  0.387]
 [ 0.007  0.068  0.11  -0.241 -0.353 -0.032]
 [ 0.384  0.385  0.442  0.407  0.453  0.385]
 [-0.199 -0.098 -0.275  0.165  0.185 -0.273]] (array([0., 0., 1., 0., 0., 0.]), array([ 0.,  1.,  0., -0.,  0.,  0.]))
[ 0.  1.  0. -0.  0.  0.]
Frame: 420000, Avg. Length: 532.7, P0 Avg. Reward: -0.386, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.386, P1 Avg. RL Loss: 0.010
Frame: 450000, Avg. Length: 533.3, P0 Avg. Reward: -0.019, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.019, P1 Avg. RL Loss: 0.010
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[-0.226 -0.205  0.19  -0.014 -0.093 -0.205]
 [-0.235 -0.203  0.012 -0.038 -0.113 -0.223]
 [-0.242 -0.206 -0.058  0.077 -0.104 -0.225]
 [-0.355 -0.289  0.05  -0.176 -0.342 -0.325]
 [-0.228 -0.194  0.023  0.052 -0.054 -0.185]
 [-0.366 -0.352 -0.435  0.3   -0.039 -0.417]] (array([1., 0., 0., 0., 0., 0.]), array([ 1., -0.,  0.,  0.,  0.,  0.]))
[ 1. -0.  0.  0.  0.  0.]
Frame: 480000, Avg. Length: 519.1, P0 Avg. Reward: -0.491, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.491, P1 Avg. RL Loss: 0.010
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[ 0.535  0.528  0.529  0.522  0.567  0.555]
 [ 0.534  0.532  0.534  0.548  0.575  0.544]
 [ 0.516  0.511  0.512  0.527  0.513  0.527]
 [ 0.053  0.1    0.151 -0.311 -0.46  -0.081]
 [ 0.536  0.537  0.568  0.551  0.599  0.537]
 [-0.252 -0.126 -0.339  0.194  0.259 -0.392]] (array([0., 0., 0., 0., 1., 0.]), array([ 1., -0.,  0.,  0.,  0.,  0.]))
[ 1. -0.  0.  0.  0.  0.]
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[ 0.692  0.691  0.718  0.72   0.724  0.69 ]
 [ 0.792  0.671  0.699  0.747  0.734  0.75 ]
 [ 0.763  0.703  0.727  0.71   0.708  0.708]
 [ 0.123  0.151  0.17  -0.38  -0.539 -0.09 ]
 [ 0.715  0.691  0.699  0.686  0.717  0.717]
 [-0.276 -0.18  -0.442  0.243  0.365 -0.437]] (array([0., 0., 1., 0., 0., 0.]), array([ 0.,  1.,  0.,  0., -0.,  0.]))
[ 0.  1.  0.  0. -0.  0.]
Frame: 510000, Avg. Length: 531.6, P0 Avg. Reward: -0.228, P0 Avg. RL Loss: nan, P1 Avg. Reward: 0.228, P1 Avg. RL Loss: 0.010
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[ 0.816  0.806  0.845  0.894  0.803  0.806]
 [ 0.878  0.755  0.799  0.855  0.791  0.824]
 [ 0.812  0.788  0.801  0.738  0.799  0.766]
 [ 0.133  0.188  0.173 -0.438 -0.615 -0.122]
 [ 0.796  0.743  0.796  0.784  0.777  0.772]
 [-0.258 -0.23  -0.497  0.244  0.439 -0.498]] (array([1., 0., 0., 0., 0., 0.]), array([ 0., -0.,  0.,  0.,  1.,  0.]))
[ 0. -0.  0.  0.  1.  0.]
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[ 0.861  0.83   0.826  0.809  0.851  0.826]
 [ 0.834  0.854  0.811  0.833  0.864  0.852]
 [ 0.785  0.843  0.81   0.824  0.86   0.759]
 [ 0.115  0.173  0.226 -0.463 -0.707 -0.155]
 [ 0.853  0.854  0.841  0.853  0.875  0.855]
 [-0.336 -0.174 -0.486  0.241  0.417 -0.633]] (array([0., 0., 0., 0., 1., 0.]), array([-0.,  0.,  1.,  0.,  0.,  0.]))
[-0.  0.  1.  0.  0.  0.]
Not a valid distribution from Nash equilibrium solution.
1.0 1.0
[[-0.567 -0.603  0.11  -0.186 -0.436 -0.616]
 [-0.613 -0.612 -0.279 -0.325 -0.541 -0.613]
 [-0.606 -0.605 -0.39  -0.15  -0.443 -0.621]
 [-0.63  -0.619 -0.045 -0.189 -0.393 -0.607]
 [-0.635 -0.681 -0.234 -0.155 -0.41  -0.729]
 [-0.596 -0.596 -0.572  0.482 -0.246 -0.62 ]] (array([0.18 , 0.   , 0.   , 0.607, 0.   , 0.213]), array([-0.   ,  0.358,  0.   ,  0.   ,  0.   ,  0.642]))
[-0.     0.358  0.     0.     0.     0.642]
Frame: 540000, Avg. Length: 589.2, P0 Avg. Reward: 0.157, P0 Avg. RL Loss: nan, P1 Avg. Reward: -0.157, P1 Avg. RL Loss: 0.011Load SlimeVolley env: SlimeVolley-v0
Load SlimeVolley env: SlimeVolley-v0
Load SlimeVolley env: SlimeVolley-v0
