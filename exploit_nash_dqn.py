import torch
import torch.optim as optim
import torch.nn.functional as F

import time, os
import random
import numpy as np
from collections import deque
from tensorboardX import SummaryWriter

from model import DQN, Policy
from storage import ReplayBuffer, ReservoirBuffer
from common.utils import epsilon_scheduler, update_target, print_log, load_model, save_model
from common.utils import create_log_dir, print_args, set_global_seeds
from common.wrappers import wrap_pytorch, make_env
from arguments import get_args
from nash_dqn import ParallelNashAgent
from evaluate_metadata import model_metadata


def exploit(env, evaluated_model, exploiter, args, exploiter_path, num_agents=2): 
    print(exploiter_path)
    [n0, n1] = env.agents # agents name in one env, 2-player game
    epsilon_by_frame = epsilon_scheduler(args.eps_start, args.eps_final, args.eps_decay)

    evaluated_model_idx = 0
    exploiter_idx = 1
    reward_list, length_list, rl_loss_list = [], [], []
    last_epi_frame_idx = 0

    states = env.reset()
    for frame_idx in range(1, args.max_frames + 1):
        epsilon = epsilon_by_frame(frame_idx)
        # actions_ = evaluated_model.act(states.reshape(states.shape[0], -1), 0.)  # for more than 1 env
        eval_actions_ = evaluated_model.act(torch.FloatTensor(states).reshape(-1).unsqueeze(0), 0.)[0]  # epsilon=0, greedy action
        exploiter_state = states[exploiter_idx]
        exploiter_action = exploiter.act(torch.FloatTensor(exploiter_state).to(args.device), epsilon)
        eval_action = eval_actions_[evaluated_model_idx]
        actions = {n0: eval_action, n1: exploiter_action}
        next_states, reward, done, _ = env.step(actions)
        exploiter_next_state = next_states[exploiter_idx]
        exploiter_reward = reward[exploiter_idx]
        reward_list.append(exploiter_reward)

        exploiter.push(exploiter_state, exploiter_action, exploiter_reward, exploiter_next_state, done)
        states = next_states
        if args.render:
            env.render() 
            # time.sleep(0.05)

        if done:
            length_list.append(frame_idx-last_epi_frame_idx)
            last_epi_frame_idx=frame_idx
            states = env.reset()

        if (len(exploiter.replay_buffer) > args.rl_start and
            frame_idx % args.train_freq == 0):
            loss = exploiter.update()
            rl_loss_list.append(loss)

        if frame_idx % args.update_target == 0:
            update_target(exploiter.model, exploiter.target)

        if frame_idx % args.evaluation_interval == 0:
            print(f"Frame: {frame_idx}, Avg. Reward: {np.mean(reward_list):.3f}, Avg. RL Loss: {np.mean(rl_loss_list):.3f}, Avg. Length: {np.mean(length_list):.1f}")
            reward_list.clear(), length_list.clear()
            rl_loss_list.clear()
            
            exploiter.save_model(exploiter_path)

class DQNTrainer(): 
    def __init__(self, env, args):
        super(DQNTrainer).__init__()
        self.model = DQN(env, args, Nash=False).to(args.device)
        self.target = DQN(env, args, Nash=False).to(args.device)
        self.replay_buffer = ReplayBuffer(args.buffer_size)
        self.optimizer = optim.Adam(self.model.parameters(), lr=args.lr)
        self.args = args

    def push(self, s, a, r, s_, d):
        self.replay_buffer.push(s, a, r, s_, np.float32(d))

    def update(self):
        state, action, reward, next_state, done = self.replay_buffer.sample(self.args.batch_size)

        state = torch.FloatTensor(np.float32(state)).to(self.args.device)
        next_state = torch.FloatTensor(np.float32(next_state)).to(self.args.device)
        action = torch.LongTensor(action).to(self.args.device)
        reward = torch.FloatTensor(reward).to(self.args.device)
        done = torch.FloatTensor(done).to(self.args.device)

        # Q-Learning with target network
        q_values = self.model(state)
        target_next_q_values = self.target(next_state)

        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)
        next_q_value = target_next_q_values.max(1)[0]
        expected_q_value = reward + (self.args.gamma ** self.args.multi_step) * next_q_value * (1 - done)

        # Huber Loss
        loss = F.smooth_l1_loss(q_value, expected_q_value.detach(), reduction='none')
        loss = loss.mean()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def act(self, s, args):
        return self.model.act(s, args)

    def save_model(self, model_path):
        torch.save(self.model.state_dict(), model_path+'dqn')
        torch.save(self.target.state_dict(), model_path+'dqn_target')

def load_exploiter(env, type, args):
    if type == 'DQN':
        agent = DQNTrainer(env, args)
        return agent

def load_evaluated_model(env, type, args):
    if type == 'NashDQN':
        agent = ParallelNashAgent(env, args)
        return agent


def main():
    Exploiter = 'DQN'
    EvaluatedModel = 'NashDQN'

    args = get_args()
    # args.against_baseline=False
    print_args(args)

    env = make_env(args)  # "SlimeVolley-v0", "SlimeVolleyPixel-v0" 'Pong-ram-v0'
    print(env.observation_space, env.action_space)

    model_prefix = model_metadata[args.env]

    exploiter = load_exploiter(env, Exploiter, args)
    evaluated_model = load_evaluated_model(env, EvaluatedModel, args)

    model_dir = "models/nash_dqn/{}/{}/".format(args.env, model_prefix)
    exploiter_dir = "models/nash_dqn/{}/{}/exploiter/".format(args.env, model_prefix)
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(exploiter_dir, exist_ok=True)

    log_dir = create_log_dir(args)
    if not args.evaluate:
        writer = SummaryWriter(log_dir)

    set_global_seeds(args.seed)
    env.seed(args.seed)

    # Parse all models saved during training in order
    filelist, epi_list = [], []
    for filename in os.listdir(model_dir):
        if filename.endswith("dqn"):
            filelist.append(filename.split('_')[0]+'_')  # remove '_dqn' at end
            epi_list.append(int(filename.split('_')[0]))
    sort_idx = np.argsort(epi_list).tolist()
    filelist = [x for _,x in sorted(zip(epi_list,filelist))] # sort filelist according to the sorting of epi_list
    epi_list.sort()  # filelist.sort() will not give correct answer   
    print(epi_list)

    # Evaluate/exploit all models saved during training in order
    eval_data={}
    for f, i in zip(filelist, epi_list):
        print('load model: ', i, model_dir, f)
        # if i>17000: 
        evaluated_model.load_model(model_dir+f, eval=True, map_location='cuda:0')  
        exploiter_path = exploiter_dir+f

        r, l = exploit(env, evaluated_model, exploiter, args, exploiter_path=exploiter_path)
        eval_data[str(i)]=[r, l]
    save_dir = 'data/{}/'.format(args.env)
    os.makedirs(save_dir, exist_ok=True)
    if args.fictitious:
        save_dir+='/fictitious_eval_data.npy'
    else:
        save_dir+='/eval_data.npy'
    np.save(save_dir, eval_data)

    writer.close()
    env.close()

if __name__ == "__main__":
    main()
